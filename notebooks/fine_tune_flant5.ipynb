{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5802a57-90d3-4067-8738-2410b607adcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# created on GCP Vertex AI Notebook using  `Python 3 (CUDA Toolkit 11.0)` environment\n",
    "# using n1-standard-4 (4 vCPUS, 15 GB RAM) compute w/ 1 NVIDIA T4 GPU\n",
    "\n",
    "# dependencies\n",
    "# %pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "# %pip install transformers datasets evaluate rouge-score nltk py7zr\n",
    "# nltk.download(\"punkt\")\n",
    "\n",
    "# note: installing an older version of pytorch so that cuda versions match\n",
    "# note: py7zr is needed for the `samsum` dataset, may or may not be needed for other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bed616b-6ef0-427b-9c91-f8f50aefb13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b828b86-80d5-40f6-8e24-31d42a56d5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a51913d-52e1-40e0-85dc-b918d66c96cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla T4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a893dc-be47-489c-85a6-6937a905f33c",
   "metadata": {},
   "source": [
    "## Notebook Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b82aac51-b46c-466b-9187-14bc85bc607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = \"/home/jupyter/data/transformers\"\n",
    "SEED = 0\n",
    "N_SAMPLES = 100\n",
    "model_name = \"google/flan-t5-base\"\n",
    "dataset_name = \"samsum\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a180d9-8972-4622-8591-03c647db5df3",
   "metadata": {},
   "source": [
    "## Load Data, Tokenizer, Model, and Evaluation Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2093f68d-fa63-4a1d-bd99-9518ef91beb0",
   "metadata": {},
   "source": [
    "dialogueWill be using the `samsum` dataset, which contains text message conversations and their summarizations\n",
    "\n",
    "https://huggingface.co/datasets/samsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74cd1fcf-59ed-4deb-bf7e-a27ea5addab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (/home/jupyter/data/transformers/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409d2b496de847b893217f587e4a8fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the dataset with custom cache location\n",
    "# cache location will prevent re-downloading the dataset everytime the notebook runs\n",
    "dataset = load_dataset(dataset_name, cache_dir=CACHE_DIR)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5772f72-3da9-43b1-a7ac-0ff3734dddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using model name, we can get the appropriate tokenizer to process inputs into\n",
    "# a format that the model expects\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "376dd43d-78e0-4506-8106-f6a63ee13f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be fine-tuning the `google/flan-t5-base` model using the above datase\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f1006-4082-46b8-b8b8-70c0cd08fe0c",
   "metadata": {},
   "source": [
    "The most commonly used metrics to evaluate summarization task is [rogue_score](https://en.wikipedia.org/wiki/ROUGE_(metric)) short for Recall-Oriented Understudy for Gisting Evaluation). This metric does not behave like the standard accuracy: it will compare a generated summary against a set of reference summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97d90d34-700c-43c7-85d1-2051670a68ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rouge will be used to evaluate summarization\n",
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7ce6d4-d64e-4f4b-bf9d-179ef570b5a7",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3089405-5498-4721-b9c8-f1b3d6c94c01",
   "metadata": {},
   "source": [
    "Read more about padding and truncation when using the tokenizer here: \\\n",
    "https://huggingface.co/docs/transformers/pad_truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ef2296e-5aa3-4e34-951b-bfb613ee7d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sample):\n",
    "    \"\"\"Tokenization function to be applied to dataset\"\"\"\n",
    "    \n",
    "    # t5 input requires a prompt prefix that specifies the task\n",
    "    prefixed_input = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
    "    \n",
    "    # tokenize inputs\n",
    "    # note that padding is left out here because it will be left to the data collator\n",
    "    model_inputs = tokenizer(text=prefixed_input, truncation=True)\n",
    "    \n",
    "    # tokenizing labels using `text_target` argument\n",
    "    # note that padding is left out here because it will be left to the data collator\n",
    "    labels = tokenizer(text_target=sample[\"summary\"], truncation=True)\n",
    "    \n",
    "    # `labels` is a required name for pytorch evaluation\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7839221-6e90-43f7-9d4b-db437d2ab03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jupyter/data/transformers/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-79ed9ef5fd96705e.arrow\n",
      "Loading cached processed dataset at /home/jupyter/data/transformers/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-b81ee4e2b0b6277f.arrow\n",
      "Loading cached processed dataset at /home/jupyter/data/transformers/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-e1c8959da63e034a.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying preprocess function to entire dataset\n",
    "# note 1: had the tokenizers had padding=True, all observations in the dataset would have been padded/truncatd to the same length, regardless of how they are batched\n",
    "# note 2: this creates new column, and the `map` method takes an arguments to remove unneeded columns\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True, remove_columns=[\"id\", \"dialogue\", \"summary\"])\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c11624e-d707-44ae-a1f6-2d8f16cf376a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~original inputs~~~~~\n",
      "Julius: dude, your assessment of manutd\n",
      "Lawrence: i have nothing to say, im so offended and hopeless of them this season\n",
      "Julius: me too\n",
      "Lawrence: i dont even know whats wrong with the team\n",
      "Julius: the quality is there but nothing is happening\n",
      "Lawrence: the players look tired of something\n",
      "Julius:  with mourinhos conservative football!!\n",
      "Lawrence: its so boring\n",
      "Julius: so lifeless\n",
      "Lawrence: man!!\n",
      "Julius: it needs to change, hope the board sees it\n",
      "Lawrence: sooner than later\n",
      "Julius: yeah\n",
      "Lawrence: yeah\n",
      "~~~~encoded inputs~~~~~\n",
      "[21603, 10, 9983, 302, 10, 146, 221, 6, 39, 4193, 13, 954, 4796, 26, 16617, 10, 3, 23, 43, 1327, 12, 497, 6, 256, 78, 326, 14550, 11, 897, 924, 13, 135, 48, 774, 9983, 302, 10, 140, 396, 16617, 10, 3, 23, 2483, 237, 214, 125, 7, 1786, 28, 8, 372, 9983, 302, 10, 8, 463, 19, 132, 68, 1327, 19, 4626, 16617, 10, 8, 1508, 320, 7718, 13, 424, 9983, 302, 10, 28, 3, 51, 1211, 23738, 7, 11252, 3370, 1603, 16617, 10, 165, 78, 13006, 9983, 302, 10, 78, 280, 924, 16617, 10, 388, 1603, 9983, 302, 10, 34, 523, 12, 483, 6, 897, 8, 1476, 217, 7, 34, 16617, 10, 14159, 145, 865, 9983, 302, 10, 17945, 16617, 10, 17945, 1]\n",
      "~~~~decoded inputs~~~~~\n",
      "summarize: Julius: dude, your assessment of manutd Lawrence: i have nothing to say, im so offended and hopeless of them this season Julius: me too Lawrence: i dont even know whats wrong with the team Julius: the quality is there but nothing is happening Lawrence: the players look tired of something Julius: with mourinhos conservative football!! Lawrence: its so boring Julius: so lifeless Lawrence: man!! Julius: it needs to change, hope the board sees it Lawrence: sooner than later Julius: yeah Lawrence: yeah</s>\n",
      "~~~~encoded targets~~~~~\n",
      "[16617, 744, 31, 17, 114, 8, 577, 13, 9145, 907, 5, 216, 11, 9983, 302, 15524, 81, 8, 372, 11, 283, 1211, 23738, 31, 7, 869, 5, 1]\n",
      "~~~~decoded inputs~~~~~\n",
      "Lawrence doesn't like the play of Manchester United. He and Julius complain about the team and Mourinho's style.</s>\n",
      "~~~~sample length in batch~~~~~\n",
      "[125, 91]\n"
     ]
    }
   ],
   "source": [
    "# giving example of how data looks raw, then tokenized, then decoded\n",
    "# note again, there is no padding here\n",
    "sample = tokenized_dataset[\"train\"][25:27]\n",
    "\n",
    "print(\"~~~~original inputs~~~~~\")\n",
    "print(dataset[\"train\"][\"dialogue\"][25])\n",
    "\n",
    "print(\"~~~~encoded inputs~~~~~\")\n",
    "print(sample[\"input_ids\"][0])\n",
    "\n",
    "print(\"~~~~decoded inputs~~~~~\")\n",
    "print(tokenizer.decode(sample[\"input_ids\"][0]))\n",
    "\n",
    "print(\"~~~~encoded targets~~~~~\")\n",
    "print(sample[\"labels\"][0])\n",
    "\n",
    "print(\"~~~~decoded inputs~~~~~\")\n",
    "print(tokenizer.decode(sample[\"labels\"][0]))\n",
    "\n",
    "print(\"~~~~sample length in batch~~~~~\")\n",
    "print([len(x) for x in sample[\"input_ids\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37e046e-2f3d-4ef8-99b8-ccf984f558ce",
   "metadata": {},
   "source": [
    "## Fine Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb3cfae-f12e-4c60-8993-d131d631db3d",
   "metadata": {},
   "source": [
    "Have to create a function that performs the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60c1e8de-4519-46d2-bd37-8bca7cf208fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # predictions have to be decoded into tokens\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    # returns a dictionary metric: score pairs\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    \n",
    "    # Extract a few results\n",
    "    result = {key: value for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length, will be shown during training loop output\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5d30566-d2a8-4214-b2b8-e59278b08ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamically padding the inputs for each batch, as oppose to padding to the max of the entire dataset\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model_name,\n",
    "    padding=True,\n",
    "    label_pad_token_id=-100 # pytorch ignores during loss when label ids are -100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1244bffa-73cc-4c10-9b41-787849114d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"flan-t5-base-tune/\",\n",
    "    per_device_train_batch_size=8, # important for avoiding OOM\n",
    "    per_device_eval_batch_size=8, # important for avoiding OOM\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, # setting to true here produces NaNs in evaluation for some reason\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27855b65-1725-4d3a-a397-38791ea2ccc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/jupyter/data/transformers/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-7bfdb573b34d23fd.arrow\n",
      "Loading cached shuffled indices for dataset at /home/jupyter/data/transformers/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-d0e26d897b4501cc.arrow\n"
     ]
    }
   ],
   "source": [
    "# creating smaller training and test samples to speed up training\n",
    "# this is optional, though recommended to see if testing is working without errors before scaling up ot full dataset\n",
    "small_train = tokenized_dataset[\"train\"].shuffle(seed=SEED).select(range(500))\n",
    "small_test = tokenized_dataset[\"test\"].shuffle(seed=SEED).select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecf403c8-efe0-41dd-8b50-95eca607010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=small_train, # replace with tokenized_dataset[\"train\"] if want to use full dataset\n",
    "    eval_dataset=small_test, # replace with tokenized_dataset[\"test\"] if want to use full dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54083c86-94e6-40c2-bff2-8b61adcd9a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [315/315 10:04, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.510700</td>\n",
       "      <td>1.427117</td>\n",
       "      <td>0.459700</td>\n",
       "      <td>0.225500</td>\n",
       "      <td>0.384900</td>\n",
       "      <td>0.425700</td>\n",
       "      <td>17.142000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.381900</td>\n",
       "      <td>1.429349</td>\n",
       "      <td>0.461000</td>\n",
       "      <td>0.224700</td>\n",
       "      <td>0.385700</td>\n",
       "      <td>0.423500</td>\n",
       "      <td>17.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.283100</td>\n",
       "      <td>1.444056</td>\n",
       "      <td>0.457900</td>\n",
       "      <td>0.222300</td>\n",
       "      <td>0.381100</td>\n",
       "      <td>0.422700</td>\n",
       "      <td>17.408000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.236500</td>\n",
       "      <td>1.448948</td>\n",
       "      <td>0.461000</td>\n",
       "      <td>0.227200</td>\n",
       "      <td>0.386100</td>\n",
       "      <td>0.425100</td>\n",
       "      <td>17.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.196800</td>\n",
       "      <td>1.452174</td>\n",
       "      <td>0.461500</td>\n",
       "      <td>0.226800</td>\n",
       "      <td>0.384700</td>\n",
       "      <td>0.425100</td>\n",
       "      <td>17.374000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 606.22\n",
      "Samples/second: 4.12\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "result = trainer.train()\n",
    "\n",
    "print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "670bf0c4-de6d-49a3-98a1-81d6ff61baff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.4271172285079956,\n",
       " 'eval_rouge1': 0.4597,\n",
       " 'eval_rouge2': 0.2255,\n",
       " 'eval_rougeL': 0.3849,\n",
       " 'eval_rougeLsum': 0.4257,\n",
       " 'eval_gen_len': 17.142,\n",
       " 'eval_runtime': 53.5001,\n",
       " 'eval_samples_per_second': 9.346,\n",
       " 'eval_steps_per_second': 1.178,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluating best model on the test set\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c5df1220-8f5a-4b48-b860-dde776a4f4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model to local directory\n",
    "trainer.save_model(\"flan-t5-based-tuned-to-max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52a2f2e-082d-457f-9507-030ead295dff",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c4bce6-a5fc-4ac2-a0b3-4840ee834442",
   "metadata": {},
   "source": [
    "https://www.philschmid.de/fine-tune-flan-t5  \n",
    "https://huggingface.co/course/chapter7/5?fw=pt"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m107",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m107"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
