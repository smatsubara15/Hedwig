{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Model Pip**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import getpass\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CPUs: 8\n",
      "INFO: Pandarallel will run on 7 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast  # Import the ast module for literal evaluation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "import multiprocessing\n",
    "num_processors = multiprocessing.cpu_count()\n",
    "\n",
    "import pandarallel\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(nb_workers=num_processors-1, use_memory_fs=False)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LANGCHAIN\n",
    "import langchain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.memory import SimpleMemory\n",
    "\n",
    "#CHROMA\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Setting up the chroma client\n",
    "chroma_client = chromadb.PersistentClient(path=\"vectorstores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "-----\n",
    "## Entire Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_messages=pd.read_csv('human_validation_with_relevent_date.csv', parse_dates=['sender_date','replier_date'])\n",
    "df_messages.dropna(subset=['sender'], axis=0, inplace=True)\n",
    "df_messages.rename(columns={'Sender_Receiver_Emails':'Replier_Emails_Sender', 'Sender_Emails_All':'Replier_Emails_All'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_messages=pd.read_csv('Hedwig/07_HumanValidation/20231104_human_validation_dataset.csv')\n",
    "df_messages['Replier_Emails_Sender'] = df_messages['Replier_Emails_Sender'].apply(lambda x: ast.literal_eval(x) if pd.notna(x) else [])\n",
    "df_messages['num_emails_toSender'] = df_messages['Replier_Emails_Sender'].apply(lambda x: len(x) if isinstance(x, list) else np.nan) + 1\n",
    "df_messages['Replier_Emails_All'] = df_messages['Replier_Emails_All'].apply(lambda x: ast.literal_eval(x) if pd.notna(x) else [])\n",
    "df_messages['num_emails_all'] = df_messages['Replier_Emails_All'].apply(lambda x: len(x) if isinstance(x, list) else np.nan) + 1\n",
    "df_messages['sender_replier_thread'] = df_messages['sender'].str.cat(df_messages['replier'], sep='-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Enter API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OpenAI API Key: ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key: is set\n"
     ]
    }
   ],
   "source": [
    "os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')\n",
    "\n",
    "if 'OPENAI_API_KEY' in os.environ:\n",
    "    openai_api_key = os.environ['OPENAI_API_KEY']\n",
    "    print(\"OpenAI API Key: is set\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY environment variable is not set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "-----\n",
    "## User Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Email Environment\n",
    "sender_id='Kshitij'\n",
    "replier_id='Scott'\n",
    "\n",
    "subject_email='New Member Onboarding'\n",
    "sender_email='Hey Scott, were you able to check if Aarushi would be available for the Friday meeting?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset=df_messages\n",
    "vector_db_client=chroma_client # FOR RANKING VECTOR DATABASE\n",
    "num_emails = 5 # Constant number of emails being retrieved for MMR, Threads, Past Emails\n",
    "\n",
    "# TEXT GENERATION CONTROL\n",
    "api_key=openai_api_key\n",
    "llm_model='gpt-3.5-turbo-0301' # CAN CHANGE\n",
    "llm_endpoint=ChatOpenAI(temperature=0.1, model=llm_model, openai_api_key=api_key) # CAN CHANGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Setting up Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "sender_name=sender_id\n",
    "replier_name=replier_id\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # Define a translation table to remove punctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    # Use the translate method of the string class to remove any punctuation\n",
    "    return text.translate(translator)\n",
    "\n",
    "def get_tokens(replier_id,\n",
    "               df):\n",
    "    user_df = df[(df.replier==replier_id)]\n",
    "    # Tokenize each entry\n",
    "    # user_df['cleaned_message'] = user_df['reply_message'].apply(lambda x: text.translate(str.maketrans('', '', string.punctuation)))\n",
    "    user_df['tokens'] = user_df['reply_message'].apply(lambda x: word_tokenize(remove_punctuation(x)))\n",
    "    user_df['token_count'] = user_df['tokens'].apply(lambda x: len(x))\n",
    "    # Calculate the average number of tokens\n",
    "    average_tokens = user_df['token_count'].median()\n",
    "    return average_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---- -\n",
    "## Zeroeth LLM Endpoint - Running MMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Langchain vectorstore using chroma collections\n",
    "user_vector_store = Chroma(\n",
    "    client=vector_db_client, \n",
    "    collection_name='user'+str(replier_id),\n",
    "    embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "    # Getting ranked responses using MMR\n",
    "found_rel_emails = await user_vector_store.amax_marginal_relevance_search(sender_email, k=num_emails, fetch_k=num_emails)\n",
    "list_rel_emails=[]\n",
    "for i, doc in enumerate(found_rel_emails):\n",
    "    list_rel_emails.append(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## First LLM Endpoint - Global Context Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT\n",
    "template_string_globalcontext=\"\"\"You are the person recieving this email {sender_email},\n",
    "Write a reply to the email as the person who recieved it, \n",
    "deriving context and writing style and email length from previous relevant emails from the person given: {relevant_emails}, \n",
    "Make sure to use salutation and signature style similar to the revelant emails above.\n",
    "You are replying to {sender_name} on behalf of {replier_name}.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting up LangChain\n",
    "prompt_template_globalcontext = ChatPromptTemplate.from_template(template=template_string_globalcontext)    \n",
    "llm_chain_globalcontext=LLMChain(llm=llm_endpoint, prompt=prompt_template_globalcontext, output_key='Global_Context_Email')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "## Second LLM Endpoint - Thread (Local Context Email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inputs from Data\n",
    "\n",
    "import math\n",
    "def get_threads(sender,\n",
    "                replier,\n",
    "                subject,\n",
    "                df,\n",
    "                num_emails_past):\n",
    "    relevant_df = df[((df.sender==sender) & (df.replier==replier) & (df.subject == subject))]\n",
    "    \n",
    "    if (len(relevant_df)==0):\n",
    "        relevant_df = df[((df.sender==replier) & (df.replier==sender) & (df.subject == subject))]\n",
    "        \n",
    "    if (len(relevant_df)==0):\n",
    "        return\n",
    "    \n",
    "    relevant_df['sender_date'] = pd.to_datetime(relevant_df['sender_date'])\n",
    "    relevant_df['replier_date'] = pd.to_datetime(relevant_df['replier_date'])\n",
    "    \n",
    "    messages = pd.concat([relevant_df['message'], relevant_df['reply_message']]).reset_index(drop=True)\n",
    "    dates = pd.concat([relevant_df['sender_date'], relevant_df['replier_date']]).reset_index(drop=True)\n",
    "    name = pd.concat([relevant_df['sender'], relevant_df['replier']]).reset_index(drop=True)\n",
    "    \n",
    "    thread_df = pd.DataFrame({'message': messages,'date': dates,'name':name})\n",
    "    thread_df = thread_df.sort_values(by='date',ascending=False)\n",
    "    \n",
    "    ordered_names = list(thread_df.name)\n",
    "    ordered_messages = list(thread_df.message)\n",
    "    \n",
    "    thread_string = ''\n",
    "    for i in range(num_emails_past):\n",
    "        thread_string = thread_string + f\"{ordered_names[i]} Email {math.ceil((i+1)/2)}: {ordered_messages[i]} \\n \\n\"\n",
    "        \n",
    "    # print(thread_string)\n",
    "    return thread_string\n",
    "\n",
    "past_threads=get_threads(sender=sender_id,\n",
    "            replier=replier_id,\n",
    "            subject=subject_email,\n",
    "            df=base_dataset,\n",
    "            num_emails_past=2*num_emails)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_string_thread=\"\"\"Take this LLM generated email: {Global_Context_Email}. \n",
    "This email might have some trailing emails, stored in the email thread here: {past_threads}.\n",
    "Rewrite the LLM Generated Email, by deprioritizing topics which are not present in the past email thread.\n",
    "Otherwise don't make major changes to the LLM generated email\"\"\"\n",
    "\n",
    "prompt_template_thread=ChatPromptTemplate.from_template(template=template_string_thread)\n",
    "llm_chain_thread=LLMChain(llm=llm_endpoint, prompt=prompt_template_thread, output_key='Local_Context_Email')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "-----\n",
    "## Third LLM Chain - Extracting Pairwise Writing Style Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs From Data\n",
    "\n",
    "def get_replier_sender_past_emails(sender,\n",
    "                                   replier,\n",
    "                                   df,\n",
    "                                   num_past_emails):\n",
    "    \n",
    "    relevant_df = df[(((df.sender==sender) & (df.replier==replier)) | ((df.sender==replier) & (df.replier==sender)))]\n",
    "    \n",
    "    relevant_df['sender_date'] = pd.to_datetime(relevant_df['sender_date'])\n",
    "    relevant_df['replier_date'] = pd.to_datetime(relevant_df['replier_date'])\n",
    "    \n",
    "    messages = pd.concat([relevant_df['message'], relevant_df['reply_message']]).reset_index(drop=True)\n",
    "    dates = pd.concat([relevant_df['sender_date'], relevant_df['replier_date']]).reset_index(drop=True)\n",
    "    name = pd.concat([relevant_df['sender'], relevant_df['replier']]).reset_index(drop=True)\n",
    "    \n",
    "    relationship_df = pd.DataFrame({'message': messages,'date': dates,'name':name})\n",
    "    relationship_df = relationship_df.sort_values(by='date',ascending=False)\n",
    "    \n",
    "    relationship_df = relationship_df[relationship_df.name==replier]\n",
    "    \n",
    "    ordered_names = list(relationship_df.name)\n",
    "    ordered_messages = list(relationship_df.message)\n",
    "    \n",
    "    past_emails_string = ''\n",
    "    for i in range(num_past_emails):\n",
    "        past_emails_string = past_emails_string + f\"Replier Email {i+1}: {ordered_messages[i]} \\n \\n\"\n",
    "        \n",
    "    return past_emails_string\n",
    "\n",
    "past_emails=get_replier_sender_past_emails(sender=sender_id,\n",
    "                               replier=replier_id,\n",
    "                               df=base_dataset,\n",
    "                               num_past_emails=num_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_string_pairstyle=\"\"\"Extract Email Writing Style in 3 words that best decribe the replier by analyzing these past emails between the sender and replier: {past_emails}\"\"\"\n",
    "\n",
    "prompt_template_pairstyle = ChatPromptTemplate.from_template(template=template_string_pairstyle)    \n",
    "llm_chain_pairstyle=LLMChain(llm=llm_endpoint, prompt=prompt_template_pairstyle, output_key='pair_style')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Fourth LLM Chain - Personalizing Local Context Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_string_personalization=\"\"\"Take this email :<{Local_Context_Email}>, update the email and create one single email which is {pair_style}. \n",
    "Remember that these adjectives collectively describe your writing style,\n",
    "DO NOT add any more information, just tweak the style a little.\n",
    "Don't be dramatic, and the output should have approximately {avg_tokens} number of tokens\"\"\"\n",
    "\n",
    "prompt_template_personalization=ChatPromptTemplate.from_template(template=template_string_personalization)\n",
    "llm_chain_personalization=LLMChain(llm=llm_endpoint, prompt=prompt_template_personalization, output_key='Personalized_Email')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sequential LLM Chain for Pair and Email Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_chain = SequentialChain(memory=SimpleMemory(memories={\"sender_name\":sender_name,\n",
    "                                                           \"replier_name\":replier_name,\n",
    "                                                           \"avg_tokens\":get_tokens(replier_id=replier_name, df=df_messages)}),\n",
    "                              chains=[llm_chain_globalcontext, llm_chain_thread,llm_chain_pairstyle, llm_chain_personalization],\n",
    "                              input_variables=['relevant_emails','sender_email','past_threads','past_emails'],\n",
    "                              output_variables=['Global_Context_Email','Local_Context_Email','pair_style','Personalized_Email']\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'relevant_emails': ['This sounds like a fun plan bro. Just to confirm, Aarushi can come right?',\n",
       "  \"Yo bro, do you think aarushi could help on our project. She's my homie and I think we could use her help\",\n",
       "  'Okay u can ask her when she gets here. So does Friday at noon work then?',\n",
       "  'For sure bro, we can do that. Do we know who is gonna be in our car yet? If Radhika isnt gonna be then we need to figure that out. I dont really care who else is as long as Aarushi is in our car? Do u think we need anything else for the trip?',\n",
       "  ':( Oh, didnt know she had a boyfriend. All good tho, she said can help with ranking our responses. She said she avaiable on Friday at noon. U cool with that?'],\n",
       " 'sender_email': 'Hey Scott, were you able to check if Aarushi would be available for the Friday meeting?',\n",
       " 'past_threads': \"Kshitij Email 1: Hey Scott, yes absolutely we can bring her! \\n \\nScott Email 1: This sounds like a fun plan bro. Just to confirm, Aarushi can come right? \\n \\nKshitij Email 2: Turtle Racing?\\n\\nThat's the first time I am hearing about it. I would absolutely love to see that, as long as its a safe environment for the turtles.\\n\\nBest,\\nKshitij \\n \\nScott Email 2: I heard that place is delish. Lets go and then maybe we can check out some turtle racing at Tin Lizzie. \\n \\nKshitij Email 3: Absolutely!\\nI have been thinking of checking out Velvet Taco in West Loop. Maybe we can drive down there post work? What bettter way of relaxing on a Friday\\n\\nBest,\\nKshitij \\n \\nScott Email 3: Alright she is down for that. She said she hopes it doesnt take that long lol. U wanna get dinner together afterwards? \\n \\nKshitij Email 4: That is indeed perfect yes!\\nLet's meet at the John Crerar Building, I have a conference room booked there between 12-4\\n\\nBest,\\nKshitij \\n \\nScott Email 4: Okay u can ask her when she gets here. So does Friday at noon work then? \\n \\nKshitij Email 5: Haha Scott, \\nYou have a great sense of humour.\\n\\nConsidering how intense her program is, along with all the PHd applications. I am sure she would enjoy this slightly non-technical work.\\n\\nLets bring her aboard for sure :)\\nBest, Kshitij \\n \\nScott Email 5: She said she doesnt wanna use her brain that much lol. She be stressed out with her work. I think thats an excuse cuz she doesnt actually know what a Rouge score is. \\n \\n\",\n",
       " 'past_emails': 'Replier Email 1: Im sure people will be fine. Im down to do that if itll help reduce the costs lol. \\n \\nReplier Email 2: Okay yeah, lets do that. Im kinda down to risk it since I like living life on the edge and I dont have any money. \\n \\nReplier Email 3: I just hope that we see the northern lights. that would be so sick.. I also hope it wont be too cold. I gotta keep track of the weather bc if its snowing while we drive I need to buy tire chains. \\n \\nReplier Email 4: Down to check out a football game while we are there, but its expensive lol. Our airbnb is super sick. Its got a hot tub, sauna, game room, and fire pit. i wanna hike, but I dont think people will wanna leave the house. \\n \\nReplier Email 5: Im down to stop in Green Bay, those nerds have a class they have to attend so we can have fun while they do that. Is the museum free? I aint got money lol \\n \\n',\n",
       " 'sender_name': 'Kshitij',\n",
       " 'replier_name': 'Scott',\n",
       " 'avg_tokens': 34.0,\n",
       " 'Global_Context_Email': \"Hey Kshitij,\\n\\nScott checked with Aarushi and she confirmed that she is available for the Friday meeting. So, we're good to go!\\n\\nThanks for checking in.\\n\\nBest,\\nScott\",\n",
       " 'Local_Context_Email': \"Hey Kshitij,\\n\\nJust wanted to let you know that Aarushi confirmed she is available for the Friday meeting. So, we're good to go!\\n\\nThanks for checking in.\\n\\nBest,\\nScott.\",\n",
       " 'pair_style': 'Casual, adventurous, budget-conscious.',\n",
       " 'Personalized_Email': \"Hey Kshitij,\\n\\nGuess what? Aarushi is totally in for the Friday meeting! We're all set to rock and roll!\\n\\nThanks for keeping me in the loop.\\n\\nCheers,\\nScott.\"}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "super_chain({\"relevant_emails\": list_rel_emails, \n",
    "             \"sender_email\": sender_email,\n",
    "             \"past_threads\": past_threads,\n",
    "             \"past_emails\":past_emails})"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
