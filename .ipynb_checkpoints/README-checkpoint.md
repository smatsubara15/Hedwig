 # Hedwig.AI <img width="119" alt="image" src="https://github.com/smatsubara15/Hedwig/assets/72986557/c3f5eebf-c739-44c5-a55c-267f1bac75cd">

An LLM-based administrative assistant that mimics user writing styles and pre-writes email replies for executives, boosting professional productivity by up to 25%


## Project Motivation 
Professionals get hundreds of emails a day:
* Many go unread
* Not organized or prioritized
* Can result in missing important Tax/Federal/Bank Statement Docs

Professionals who are busy may take a long time to respond to all these emails, and the worry of having to respond can be very stressful. This is further marred by subpar Quality of responses and workstream delays.

## How can Hedwig can help? 
Being an LLM-powered administrative assistant, Hedwing handles the following:
* Pre-write emails for executives according to their personal writing styles
* Enables a personalized virtual assistant for all employees regardless 
* Ensure data security by storing within existing organizational infrastructure

## Project Development
###  Table of Contents

1. [Technical Overview](#introduction)
2. [Data Overview](#data_overview)
3. [Personalization Framework](#personalization)
4. [Langchain Framework](#langchain)
5. [Prompt Fine-Tuning](#prompt)
6. [Model Fine-Tuning](#model)
7. [Credits](#acknowledgments)


## 1. Technical Overview <a name="introduction"></a>


## 2. Data Overview <a name="data_overview"></a>
The Enron email dataset was used for project development. This email dataset contains approximately **500,000 emails** generated by employees of the Enron Corporation. It was obtained by the Federal Energy Regulatory Commission during its investigation of Enron's collapse. 


The original data was obtained has highly dense json files. 
<img width="916" alt="image" src="https://github.com/smatsubara15/Hedwig/assets/72986557/a6deceec-9092-42ed-8eb6-12e4d7dda0b1">

For our purpose, we cleaned and transmuted the data to build message-reply pairs using the following rules:
* Emails with the same subject and sender-recipient pair were grouped into threads
* Rather than focussing on an email, an email-reply conversation was emphasized
* Empty emails, or emails with just emojis/attachments were removed

**~25000 message** reply conversations were identified, which became our modeling dataset
<img width="897" alt="image" src="https://github.com/smatsubara15/Hedwig/assets/72986557/c0ccbc0f-abfa-4675-bbc9-3cc84c8521d7">


## 3. Personalization Framework <a name="personalization"></a>
The personalization framework employs three techniques for assessing **Context** and **Style** for any incoming email:

![image](https://github.com/smatsubara15/Hedwig/assets/72986557/bf6da6e9-efb6-4705-8690-292f3a095d1e)


**Context**
* Threads - Finding the most recent conversations in that thread to get local context around any incoming query
* Ranking - The incoming query is matched with all emails the replier has sent in the past to obtain the replier's views on similar past incoming emails.
(Implementation details in Langchain Framework)

<img width="867" alt="image" src="https://github.com/smatsubara15/Hedwig/assets/72986557/88d414c3-bae6-4f3d-a5ac-a2cc3789145f">


**Style**
* Retrieval - Finding the most recent conversations between the sender and replier to obtain the communication pattern between two people

## 4. Langchain Framework <a name="langchain"></a>
Our entire pipeline is divided into two Langchain pipelines:
* Personalization Elements
* Email Generation
---

#### **Personalization Elements - Ranking Langchain Pipeline**

To find emails relevant to the incoming query:
* Each User is assigned an independent vector universe
* All the emails written by that user in the past are converted to embeddings using Open-AI's *text-embedding-ada-002*
* Using a chroma vector database, each user embedding is stored for ease of access later (Setting up a chroma client)
* The incoming query is embedded using the same model
* **Maximal Marginal Relevance Search** is implemented to find the most relevant emails between the incoming query and the user vector store

<img width="992" alt="image" src="https://github.com/smatsubara15/Hedwig/assets/72986557/a713729d-593a-4f58-a144-2d2ad7581e72">


#### **Email Generation - Langchain Pipeline**
All personalization elements are combined in a langchain pipeline for email generation.
* LLM Model Used - gpt-3.5-turbo
* Temperature - 0/0.1

<img width="919" alt="image" src="https://github.com/smatsubara15/Hedwig/assets/72986557/5e07313f-515e-4d30-b405-f60810e26f05">

<img width="972" alt="image" src="https://github.com/smatsubara15/Hedwig/assets/72986557/677036f0-af41-45e5-b5ee-44ab4d85a322">



