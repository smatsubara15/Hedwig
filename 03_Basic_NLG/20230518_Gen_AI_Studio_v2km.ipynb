{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46cc8463-76a2-4ab3-b660-298db0000931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install google-cloud-aiplatform==1.25.0\n",
    "# %pip install google-api-core==1.33.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28dc367b-a088-432f-b20d-df90a51a9aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc63bb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=1jCvRnoU8PMdJSmbnnPSVZY2BM4iY7&access_type=offline&code_challenge=2HN07eWQyTCJETRdFq3Y2ugNGjlGbCRP1V-hyZDTRIo&code_challenge_method=S256\n",
      "\n",
      "\n",
      "Credentials saved to file: [/Users/scottsmacbook/.config/gcloud/application_default_credentials.json]\n",
      "\n",
      "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
      "\n",
      "Quota project \"msca310019-capstone-49b3\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n"
     ]
    }
   ],
   "source": [
    "! /Users/scottsmacbook/google-cloud-sdk/bin/gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04db96bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "def authenticate_implicit_with_adc(project_id=\"your-google-cloud-project-id\"):\n",
    "    \"\"\"\n",
    "    When interacting with Google Cloud Client libraries, the library can auto-detect the\n",
    "    credentials to use.\n",
    "\n",
    "    // TODO(Developer):\n",
    "    //  1. Before running this sample,\n",
    "    //  set up ADC as described in https://cloud.google.com/docs/authentication/external/set-up-adc\n",
    "    //  2. Replace the project variable.\n",
    "    //  3. Make sure that the user account or service account that you are using\n",
    "    //  has the required permissions. For this sample, you must have \"storage.buckets.list\".\n",
    "    Args:\n",
    "        project_id: The project id of your Google Cloud project.\n",
    "    \"\"\"\n",
    "\n",
    "    # This snippet demonstrates how to list buckets.\n",
    "    # *NOTE*: Replace the client created below with the client required for your application.\n",
    "    # Note that the credentials are not specified when constructing the client.\n",
    "    # Hence, the client library will look for credentials using ADC.\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    buckets = storage_client.list_buckets()\n",
    "    print(\"Buckets:\")\n",
    "    for bucket in buckets:\n",
    "        print(bucket.name)\n",
    "    print(\"Listed all storage buckets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8199b60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buckets:\n",
      "user-scripts-msca310019-capstone-49b3\n",
      "Listed all storage buckets.\n"
     ]
    }
   ],
   "source": [
    "authenticate_implicit_with_adc('msca310019-capstone-49b3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a44678fd-b78d-49d1-ac2c-abf35e4ba41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_large_language_model_sample(\n",
    "    project_id: str,\n",
    "    model_name: str,\n",
    "    temperature: float,\n",
    "    max_decode_steps: int,\n",
    "    top_p: float,\n",
    "    top_k: int,\n",
    "    content: str,\n",
    "    location: str = \"us-central1\",\n",
    "    tuned_model_name: str = \"\",\n",
    "    ) :\n",
    "    \"\"\"Predict using a Large Language Model.\"\"\"\n",
    "    vertexai.init(project=project_id, location=location)\n",
    "    model = TextGenerationModel.from_pretrained(model_name)\n",
    "    if tuned_model_name:\n",
    "        model = model.get_tuned_model(tuned_model_name)\n",
    "    response = model.predict(\n",
    "        content,\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=max_decode_steps,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "068a618a-eccc-483f-9b09-50cdc00833d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbfa5fbe-f91c-4293-ba7a-61972cd7cbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5e31232-1e0d-4636-be98-1da8975a2326",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''Rewrite the following article so it can be understood by 5 year old:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c649c9c5-0df2-494e-8412-b66793b42272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Model: . Computers are getting better at understanding language. They can be trained on a lot of data, and then they can be used to do things like answer your questions or write different kinds of text.\n",
      "\n",
      "But training computers to understand language is hard, and it takes a lot of time and computing power. So scientists are always looking for ways to make it faster and cheaper.\n",
      "\n",
      "One new way is called Low-Rank Adaptation, or LoRA. LoRA works by taking a pre-trained language model and then making it smaller. This makes it faster and cheaper to train, but it doesn't seem to hurt the model's ability to understand language.\n",
      "\n",
      "LoRA is still new, but it's showing a lot of promise. Scientists think it could be used to make computers even better at understanding language, and that could open up a whole new world of possibilities.\n"
     ]
    }
   ],
   "source": [
    "predict_large_language_model_sample(project_id = \"msca310019-capstone-49b3\", \n",
    "                                    model_name = \"text-bison@001\", \n",
    "                                    temperature = 0.2, \n",
    "                                    max_decode_steps = 768, \n",
    "                                    top_p = 0.8, \n",
    "                                    top_k = 40, \n",
    "                                    location = \"us-central1\",\n",
    "                                    content = prompt + ' ' + text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73852072-2fc0-4d4e-84a7-a0458e88d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''Rewrite the following article so it can be understood by 10 year old:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c32cf820-6860-4606-994b-3f422cb2e738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Model: . Computers are getting better at understanding human language. One way they do this is by learning from a lot of text. This is called \"pre-training\". Once a computer has been pre-trained, it can be \"fine-tuned\" to do a specific task, like answering your questions or writing different kinds of text.\n",
      "\n",
      "But pre-training and fine-tuning computers can be very expensive. That's why we're working on a new way to do it that's much cheaper. Our method is called \"Low-Rank Adaptation\". It works by taking the pre-trained model and making it smaller. This doesn't hurt the model's ability to do its job, but it does make it much cheaper to use.\n",
      "\n",
      "We've tested our method on a lot of different tasks and it works just as well as the old way of doing things. We're also releasing a package that makes it easy for other people to use our method.\n"
     ]
    }
   ],
   "source": [
    "predict_large_language_model_sample(project_id = \"msca310019-capstone-49b3\", \n",
    "                                    model_name = \"text-bison@001\", \n",
    "                                    temperature = 0.2, \n",
    "                                    max_decode_steps = 256, \n",
    "                                    top_p = 0.95, \n",
    "                                    top_k = 40, \n",
    "                                    location = \"us-central1\",\n",
    "                                    content = prompt + ' ' + text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2fadb1f-b605-4c47-8410-65bfc51bc088",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''Rewrite the following article so it can be understood by 15 year old:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09cc5558-fe94-4fc9-ad61-85dddd5fbca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Model: . One way to make computers understand human language is to teach them a lot of words and phrases. We do this by feeding them a large amount of text data, and then we make them guess what the next word in a sentence should be. This process is called \"pre-training\".\n",
      "\n",
      "Once a computer model has been pre-trained, we can then \"fine-tune\" it to perform a specific task. For example, we can fine-tune a pre-trained model to write different kinds of creative text, like poems or code.\n",
      "\n",
      "However, fine-tuning a large pre-trained model can be very computationally expensive. This is because the model has a lot of parameters, and we need to train all of them.\n",
      "\n",
      "To make fine-tuning more efficient, we can use a technique called \"low-rank adaptation\". This technique reduces the number of parameters in the model, without reducing the model's accuracy.\n",
      "\n",
      "We have developed a new low-rank adaptation method that works very well. Our method is called \"LoRA\". LoRA can reduce the number of parameters in a pre-trained model by a factor of 10,000, while still maintaining the model's accuracy.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_large_language_model_sample(project_id = \"msca310019-capstone-49b3\", \n",
    "                                    model_name = \"text-bison@001\", \n",
    "                                    temperature = 0.2, \n",
    "                                    max_decode_steps = 256, \n",
    "                                    top_p = 0.95, \n",
    "                                    top_k = 40, \n",
    "                                    location = \"us-central1\",\n",
    "                                    content = prompt + ' ' + text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9c712798-133b-47a6-9b32-bc40fe7cd9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_email = '''Create a brief response to the following email in a professional manner :'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ca4f57aa-2aea-4b84-bd00-cc201f9e9c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_email = 'Hey, This week is probably not a good week, but lets definately shoot for early next week.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3f0827c4-0288-4eca-9192-b1b7c156faeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Model: Hi there,\n",
      "\n",
      "Thanks for reaching out. Unfortunately, this week is pretty busy for me as well. Would you be available to chat early next week? I'm free on Monday at 10am or 2pm, or Tuesday at 10am or 3pm.\n",
      "\n",
      "Let me know what works best for you.\n",
      "\n",
      "Best,\n"
     ]
    }
   ],
   "source": [
    "predict_large_language_model_sample(project_id = \"msca310019-capstone-49b3\", \n",
    "                                    model_name = \"text-bison@001\", \n",
    "                                    temperature = 0.2, \n",
    "                                    max_decode_steps = 256, \n",
    "                                    top_p = 0.95, \n",
    "                                    top_k = 40, \n",
    "                                    location = \"us-central1\",\n",
    "                                    content = prompt_email + ' ' + text_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d776e775",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_email = '''Create a response to the following email in a professional manner : '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "345efd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_email = '''Hey Mike! Hope you had a great thanksgiving weekend. I used it to check out NYC! Anyway, \n",
    "                what's your take on the future of the firm? And how are the employees reacting so far? \n",
    "                It's difficult to recall a weekday when the WSJ does not carry an Enron related article. \n",
    "                And now it seems the merger is doubtful. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ee19f69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Model: Hi Mike,\n",
      "\n",
      "I hope you had a great Thanksgiving weekend as well! I spent mine relaxing at home with my family.\n",
      "\n",
      "As for the future of the firm, it's hard to say. The news has been pretty negative lately, and it's difficult to tell what the future holds. I think the employees are starting to feel the pressure, and morale is definitely low. The merger is definitely in doubt at this point, and it's unclear what will happen next.\n",
      "\n",
      "I'll keep you updated on what I hear.\n",
      "\n",
      "Best,\n",
      "[[your name]]\n"
     ]
    }
   ],
   "source": [
    "predict_large_language_model_sample(project_id = \"msca310019-capstone-49b3\", \n",
    "                                    model_name = \"text-bison@001\", \n",
    "                                    temperature = 0.2, \n",
    "                                    max_decode_steps = 256, \n",
    "                                    top_p = 0.95, \n",
    "                                    top_k = 40, \n",
    "                                    location = \"us-central1\",\n",
    "                                    content = prompt_email + ' ' + text_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a168c08-48e6-4429-9874-d9ad3cd822eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "\n",
    "datetime.datetime.now(pytz.timezone('US/Central')).strftime(\"%a, %d %B %Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66211f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_replies = pd.read_csv(\"/Users/scottsmacbook/capstone/00_Data/message_reply_pairs_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c67739a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>time</th>\n",
       "      <th>subject</th>\n",
       "      <th>thread</th>\n",
       "      <th>sender</th>\n",
       "      <th>recipient</th>\n",
       "      <th>message</th>\n",
       "      <th>reply_time</th>\n",
       "      <th>reply_sender</th>\n",
       "      <th>reply_recipient</th>\n",
       "      <th>reply_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1999-06-09 04:18:00-07:00</td>\n",
       "      <td>RE: test</td>\n",
       "      <td>2</td>\n",
       "      <td>5552</td>\n",
       "      <td>[40034]</td>\n",
       "      <td>How about this Friday ? Julie has not left yet...</td>\n",
       "      <td>1999-06-09 08:06:00-07:00</td>\n",
       "      <td>40034</td>\n",
       "      <td>[5552]</td>\n",
       "      <td>when? how are you and your family? is julie go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1999-06-09 08:06:00-07:00</td>\n",
       "      <td>RE: test</td>\n",
       "      <td>2</td>\n",
       "      <td>40034</td>\n",
       "      <td>[5552]</td>\n",
       "      <td>when? how are you and your family? is julie go...</td>\n",
       "      <td>1999-06-10 03:54:00-07:00</td>\n",
       "      <td>5552</td>\n",
       "      <td>[40034]</td>\n",
       "      <td>Today is bad. Tommorrow I will call you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1999-06-09 08:06:00-07:00</td>\n",
       "      <td>RE: test</td>\n",
       "      <td>2</td>\n",
       "      <td>40034</td>\n",
       "      <td>[5552]</td>\n",
       "      <td>when? how are you and your family? is julie go...</td>\n",
       "      <td>1999-11-23 01:38:00-08:00</td>\n",
       "      <td>5552</td>\n",
       "      <td>[40034]</td>\n",
       "      <td>Do you have lunch plans today?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1999-06-09 08:06:00-07:00</td>\n",
       "      <td>RE: test</td>\n",
       "      <td>2</td>\n",
       "      <td>40034</td>\n",
       "      <td>[5552]</td>\n",
       "      <td>when? how are you and your family? is julie go...</td>\n",
       "      <td>1999-11-23 03:13:00-08:00</td>\n",
       "      <td>5552</td>\n",
       "      <td>[40034]</td>\n",
       "      <td>Really? I'd feel like a mooch. Lets have lunch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1999-06-09 08:06:00-07:00</td>\n",
       "      <td>RE: test</td>\n",
       "      <td>2</td>\n",
       "      <td>40034</td>\n",
       "      <td>[5552]</td>\n",
       "      <td>when? how are you and your family? is julie go...</td>\n",
       "      <td>1999-11-23 03:58:00-08:00</td>\n",
       "      <td>5552</td>\n",
       "      <td>[40034]</td>\n",
       "      <td>Tues.is good. I'll call you.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                       time   subject  thread  sender recipient  \\\n",
       "0           0  1999-06-09 04:18:00-07:00  RE: test       2    5552   [40034]   \n",
       "1           1  1999-06-09 08:06:00-07:00  RE: test       2   40034    [5552]   \n",
       "2           2  1999-06-09 08:06:00-07:00  RE: test       2   40034    [5552]   \n",
       "3           3  1999-06-09 08:06:00-07:00  RE: test       2   40034    [5552]   \n",
       "4           4  1999-06-09 08:06:00-07:00  RE: test       2   40034    [5552]   \n",
       "\n",
       "                                             message  \\\n",
       "0  How about this Friday ? Julie has not left yet...   \n",
       "1  when? how are you and your family? is julie go...   \n",
       "2  when? how are you and your family? is julie go...   \n",
       "3  when? how are you and your family? is julie go...   \n",
       "4  when? how are you and your family? is julie go...   \n",
       "\n",
       "                  reply_time  reply_sender reply_recipient  \\\n",
       "0  1999-06-09 08:06:00-07:00         40034          [5552]   \n",
       "1  1999-06-10 03:54:00-07:00          5552         [40034]   \n",
       "2  1999-11-23 01:38:00-08:00          5552         [40034]   \n",
       "3  1999-11-23 03:13:00-08:00          5552         [40034]   \n",
       "4  1999-11-23 03:58:00-08:00          5552         [40034]   \n",
       "\n",
       "                                       reply_message  \n",
       "0  when? how are you and your family? is julie go...  \n",
       "1           Today is bad. Tommorrow I will call you.  \n",
       "2                     Do you have lunch plans today?  \n",
       "3  Really? I'd feel like a mooch. Lets have lunch...  \n",
       "4                       Tues.is good. I'll call you.  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_replies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd5d4678",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_replies['message_list'] = message_replies['message'].apply(lambda x: str(x).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98473448",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_replies_trimmed = message_replies[message_replies.message_list.apply(lambda x: len(x)>3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d6d91df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23342"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(message_replies_trimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ed4895d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24640"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(message_replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f346bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for your email. I would be happy to meet with you this Friday. Please let me know what time and location would work best for you.\n",
      "\n",
      "I am also curious to hear more about Julie's story. It sounds like she had quite the adventure!\n",
      "Hi there,\n",
      "\n",
      "Thanks for reaching out! I'm doing well, and my family is doing great. Julie is still here, but she's getting ready to move out soon.\n",
      "\n",
      "I'm free to meet up on [date] at [time] if that works for you. Let me know if you're interested.\n",
      "\n",
      "Best,\n",
      "[Your name]\n",
      "Hi [Name],\n",
      "\n",
      "Thanks for reaching out! I'm doing well, and my family is doing great. Julie is still here, but she's getting ready to move out soon.\n",
      "\n",
      "I'm free to chat on [date] at [time]. Would that work for you?\n",
      "\n",
      "Let me know,\n",
      "[Your Name]\n",
      "Hi [Name],\n",
      "\n",
      "Thanks for reaching out! I'm doing well, and my family is doing great. Julie is still here, but she's getting ready to move to [new city] next month.\n",
      "\n",
      "I'm free to chat on [date] at [time]. Would that work for you?\n",
      "\n",
      "Let me know what time works best for you, and I'll send you a link to the meeting.\n",
      "\n",
      "Looking forward to hearing from you!\n",
      "\n",
      "Best,\n",
      "[Your Name]\n",
      "Hi [Name],\n",
      "\n",
      "Thanks for reaching out! I'm doing well, and my family is doing great. Julie is still here, but she's getting ready to move out soon.\n",
      "\n",
      "I'm free to chat on [date] at [time]. Would that work for you?\n",
      "\n",
      "Let me know,\n",
      "[Your Name]\n"
     ]
    }
   ],
   "source": [
    "messages = list(message_replies_trimmed.message)\n",
    "generated_response = []\n",
    "\n",
    "for message in messages[:5]:\n",
    "    generated = predict_large_language_model_sample(project_id = \"msca310019-capstone-49b3\", \n",
    "                                    model_name = \"text-bison@001\", \n",
    "                                    temperature = 0.2, \n",
    "                                    max_decode_steps = 256, \n",
    "                                    top_p = 0.95, \n",
    "                                    top_k = 40, \n",
    "                                    location = \"us-central1\",\n",
    "                                    content = prompt_email + ' ' + message)\n",
    "    print(generated)\n",
    "    generated_response.append(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "481f773b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi [Name],\\n\\nThanks for reaching out! I'm doing well, and my family is doing great. Julie is still here, but she's getting ready to move out soon.\\n\\nI'm free to chat on [date] at [time]. Would that work for you?\\n\\nLet me know,\\n[Your Name]\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_response[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "151cd9b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'when? how are you and your family? is julie gone? '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[2]"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m107",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m107"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
