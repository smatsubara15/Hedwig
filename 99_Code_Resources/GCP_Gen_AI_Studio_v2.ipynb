{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46cc8463-76a2-4ab3-b660-298db0000931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install google-cloud-aiplatform==1.25.0\n",
    "# %pip install google-api-core==1.33.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28dc367b-a088-432f-b20d-df90a51a9aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.preview.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc63bb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=1jCvRnoU8PMdJSmbnnPSVZY2BM4iY7&access_type=offline&code_challenge=2HN07eWQyTCJETRdFq3Y2ugNGjlGbCRP1V-hyZDTRIo&code_challenge_method=S256\n",
      "\n",
      "\n",
      "Credentials saved to file: [/Users/scottsmacbook/.config/gcloud/application_default_credentials.json]\n",
      "\n",
      "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
      "\n",
      "Quota project \"msca310019-capstone-49b3\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n"
     ]
    }
   ],
   "source": [
    "! /Users/scottsmacbook/google-cloud-sdk/bin/gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04db96bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "def authenticate_implicit_with_adc(project_id=\"your-google-cloud-project-id\"):\n",
    "    \"\"\"\n",
    "    When interacting with Google Cloud Client libraries, the library can auto-detect the\n",
    "    credentials to use.\n",
    "\n",
    "    // TODO(Developer):\n",
    "    //  1. Before running this sample,\n",
    "    //  set up ADC as described in https://cloud.google.com/docs/authentication/external/set-up-adc\n",
    "    //  2. Replace the project variable.\n",
    "    //  3. Make sure that the user account or service account that you are using\n",
    "    //  has the required permissions. For this sample, you must have \"storage.buckets.list\".\n",
    "    Args:\n",
    "        project_id: The project id of your Google Cloud project.\n",
    "    \"\"\"\n",
    "\n",
    "    # This snippet demonstrates how to list buckets.\n",
    "    # *NOTE*: Replace the client created below with the client required for your application.\n",
    "    # Note that the credentials are not specified when constructing the client.\n",
    "    # Hence, the client library will look for credentials using ADC.\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    buckets = storage_client.list_buckets()\n",
    "    print(\"Buckets:\")\n",
    "    for bucket in buckets:\n",
    "        print(bucket.name)\n",
    "    print(\"Listed all storage buckets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8199b60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buckets:\n",
      "user-scripts-msca310019-capstone-49b3\n",
      "Listed all storage buckets.\n"
     ]
    }
   ],
   "source": [
    "authenticate_implicit_with_adc('msca310019-capstone-49b3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a44678fd-b78d-49d1-ac2c-abf35e4ba41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_large_language_model_sample(\n",
    "    project_id: str,\n",
    "    model_name: str,\n",
    "    temperature: float,\n",
    "    max_decode_steps: int,\n",
    "    top_p: float,\n",
    "    top_k: int,\n",
    "    content: str,\n",
    "    location: str = \"us-central1\",\n",
    "    tuned_model_name: str = \"\",\n",
    "    ) :\n",
    "    \"\"\"Predict using a Large Language Model.\"\"\"\n",
    "    vertexai.init(project=project_id, location=location)\n",
    "    model = TextGenerationModel.from_pretrained(model_name)\n",
    "    if tuned_model_name:\n",
    "        model = model.get_tuned_model(tuned_model_name)\n",
    "    response = model.predict(\n",
    "        content,\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=max_decode_steps,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,)\n",
    "    print(f\"Response from Model: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "068a618a-eccc-483f-9b09-50cdc00833d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbfa5fbe-f91c-4293-ba7a-61972cd7cbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5e31232-1e0d-4636-be98-1da8975a2326",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''Rewrite the following article so it can be understood by 5 year old:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c649c9c5-0df2-494e-8412-b66793b42272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Model: . Computers are getting better at understanding language. They can do this by learning from a lot of data. But when they learn from a lot of data, they can become too big and slow.\n",
      "\n",
      "Scientists have found a way to make computers understand language better without making them too big or slow. They do this by using a technique called \"low-rank adaptation.\" This technique allows computers to learn from a lot of data without having to store all of the data in their memory.\n",
      "\n",
      "This is important because it means that computers will be able to understand language better and faster. This could lead to new ways for computers to help us, such as by writing better emails, translating languages more accurately, and helping us to learn new things.\n"
     ]
    }
   ],
   "source": [
    "predict_large_language_model_sample(project_id = \"msca310019-capstone-49b3\", \n",
    "                                    model_name = \"text-bison@001\", \n",
    "                                    temperature = 0.2, \n",
    "                                    max_decode_steps = 768, \n",
    "                                    top_p = 0.8, \n",
    "                                    top_k = 40, \n",
    "                                    location = \"us-central1\",\n",
    "                                    content = prompt + ' ' + text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73852072-2fc0-4d4e-84a7-a0458e88d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''Rewrite the following article so it can be understood by 10 year old:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c32cf820-6860-4606-994b-3f422cb2e738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Model: . Computers are getting better at understanding human language. One way they do this is by learning from a lot of text. This is called \"pre-training\". Once a computer has been pre-trained, it can be \"fine-tuned\" to do a specific task, like answering your questions or writing different kinds of text.\n",
      "\n",
      "But pre-training and fine-tuning computers can be very expensive. That's why we're working on a new way to do it that's much cheaper. Our method is called \"Low-Rank Adaptation\". It works by taking the pre-trained model and making it smaller. This doesn't hurt the model's ability to do its job, but it does make it much cheaper to use.\n",
      "\n",
      "We've tested our method on a lot of different tasks and it works just as well as the old way of doing things. We're also releasing a package that makes it easy for other people to use our method.\n"
     ]
    }
   ],
   "source": [
    "predict_large_language_model_sample(project_id = \"msca310019-capstone-49b3\", \n",
    "                                    model_name = \"text-bison@001\", \n",
    "                                    temperature = 0.2, \n",
    "                                    max_decode_steps = 256, \n",
    "                                    top_p = 0.95, \n",
    "                                    top_k = 40, \n",
    "                                    location = \"us-central1\",\n",
    "                                    content = prompt + ' ' + text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2fadb1f-b605-4c47-8410-65bfc51bc088",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''Rewrite the following article so it can be understood by 15 year old:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09cc5558-fe94-4fc9-ad61-85dddd5fbca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Model: . One way to make computers understand human language is to teach them a lot of words and phrases. We do this by feeding them a large amount of text data, and then we make them guess what the next word in a sentence should be. This process is called \"pre-training\".\n",
      "\n",
      "Once a computer model has been pre-trained, we can then \"fine-tune\" it to perform a specific task. For example, we can fine-tune a pre-trained model to write different kinds of creative text, like poems or code.\n",
      "\n",
      "However, fine-tuning a large pre-trained model can be very computationally expensive. This is because the model has a lot of parameters, and we need to train all of them.\n",
      "\n",
      "To make fine-tuning more efficient, we can use a technique called \"low-rank adaptation\". This technique reduces the number of parameters in the model, without reducing the model's accuracy.\n",
      "\n",
      "We have developed a new low-rank adaptation method that works very well. Our method is called \"LoRA\". LoRA can reduce the number of parameters in a pre-trained model by a factor of 10,000, while still maintaining the model's accuracy.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_large_language_model_sample(project_id = \"msca310019-capstone-49b3\", \n",
    "                                    model_name = \"text-bison@001\", \n",
    "                                    temperature = 0.2, \n",
    "                                    max_decode_steps = 256, \n",
    "                                    top_p = 0.95, \n",
    "                                    top_k = 40, \n",
    "                                    location = \"us-central1\",\n",
    "                                    content = prompt + ' ' + text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9c712798-133b-47a6-9b32-bc40fe7cd9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_email = '''Create a brief response to the following email in a professional manner :'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ca4f57aa-2aea-4b84-bd00-cc201f9e9c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_email = 'Hey, This week is probably not a good week, but lets definately shoot for early next week.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3f0827c4-0288-4eca-9192-b1b7c156faeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Model: Hi there,\n",
      "\n",
      "Thanks for reaching out. Unfortunately, this week is pretty busy for me as well. Would you be available to chat early next week? I'm free on Monday at 10am or 2pm, or Tuesday at 10am or 3pm.\n",
      "\n",
      "Let me know what works best for you.\n",
      "\n",
      "Best,\n"
     ]
    }
   ],
   "source": [
    "predict_large_language_model_sample(project_id = \"msca310019-capstone-49b3\", \n",
    "                                    model_name = \"text-bison@001\", \n",
    "                                    temperature = 0.2, \n",
    "                                    max_decode_steps = 256, \n",
    "                                    top_p = 0.95, \n",
    "                                    top_k = 40, \n",
    "                                    location = \"us-central1\",\n",
    "                                    content = prompt_email + ' ' + text_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d776e775",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_email = '''Create a response to the following email in a professional manner :'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "345efd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_email = '''Hey Mike! Hope you had a great thanksgiving weekend. I used it to check out NYC! Anyway, \n",
    "                what's your take on the future of the firm? And how are the employees reacting so far? \n",
    "                It's difficult to recall a weekday when the WSJ does not carry an Enron related article. \n",
    "                And now it seems the merger is doubtful. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ee19f69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Model: Hi Mike,\n",
      "\n",
      "I hope you had a great Thanksgiving weekend as well! I spent mine relaxing at home with my family.\n",
      "\n",
      "As for the future of the firm, it's hard to say. The news has been pretty negative lately, and it's difficult to tell what the future holds. I think the employees are starting to feel the pressure, and morale is definitely low. The merger is definitely in doubt at this point, and it's unclear what will happen next.\n",
      "\n",
      "I'll keep you updated on what I hear.\n",
      "\n",
      "Best,\n",
      "[[your name]]\n"
     ]
    }
   ],
   "source": [
    "predict_large_language_model_sample(project_id = \"msca310019-capstone-49b3\", \n",
    "                                    model_name = \"text-bison@001\", \n",
    "                                    temperature = 0.2, \n",
    "                                    max_decode_steps = 256, \n",
    "                                    top_p = 0.95, \n",
    "                                    top_k = 40, \n",
    "                                    location = \"us-central1\",\n",
    "                                    content = prompt_email + ' ' + text_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a168c08-48e6-4429-9874-d9ad3cd822eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "\n",
    "datetime.datetime.now(pytz.timezone('US/Central')).strftime(\"%a, %d %B %Y %H:%M:%S\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
