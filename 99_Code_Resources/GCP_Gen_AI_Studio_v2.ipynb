{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46cc8463-76a2-4ab3-b660-298db0000931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install google-cloud-aiplatform==1.25.0\n",
    "# %pip install google-api-core==1.33.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28dc367b-a088-432f-b20d-df90a51a9aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.preview.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a44678fd-b78d-49d1-ac2c-abf35e4ba41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_large_language_model_sample(\n",
    "    project_id: str,\n",
    "    model_name: str,\n",
    "    temperature: float,\n",
    "    max_decode_steps: int,\n",
    "    top_p: float,\n",
    "    top_k: int,\n",
    "    content: str,\n",
    "    location: str = \"us-central1\",\n",
    "    tuned_model_name: str = \"\",\n",
    "    ) :\n",
    "    \"\"\"Predict using a Large Language Model.\"\"\"\n",
    "    vertexai.init(project=project_id, location=location)\n",
    "    model = TextGenerationModel.from_pretrained(model_name)\n",
    "    if tuned_model_name:\n",
    "        model = model.get_tuned_model(tuned_model_name)\n",
    "    response = model.predict(\n",
    "        content,\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=max_decode_steps,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,)\n",
    "    print(f\"Response from Model: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "068a618a-eccc-483f-9b09-50cdc00833d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbfa5fbe-f91c-4293-ba7a-61972cd7cbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5e31232-1e0d-4636-be98-1da8975a2326",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''Rewrite the following article so it can be understood by 5 year old:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c649c9c5-0df2-494e-8412-b66793b42272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Model: . Computers are getting better at understanding language. They can do this by learning from a lot of data. But when they learn from a lot of data, they can become too big and slow.\n",
      "\n",
      "Scientists have found a way to make computers understand language better without making them too big or slow. They do this by using a technique called \"low-rank adaptation.\" This technique allows computers to learn from a lot of data without having to store all of the data in their memory.\n",
      "\n",
      "This is important because it means that computers will be able to understand language better and faster. This could lead to new ways for computers to help us, such as by writing better emails, translating languages, or helping us with our homework.\n"
     ]
    }
   ],
   "source": [
    "predict_large_language_model_sample(project_id = \"msca-31013-big-data\", \n",
    "                                    model_name = \"text-bison@001\", \n",
    "                                    temperature = 0.2, \n",
    "                                    max_decode_steps = 768, \n",
    "                                    top_p = 0.8, \n",
    "                                    top_k = 40, \n",
    "                                    location = \"us-central1\",\n",
    "                                    content = prompt + ' ' + text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73852072-2fc0-4d4e-84a7-a0458e88d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''Rewrite the following article so it can be understood by 10 year old:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c32cf820-6860-4606-994b-3f422cb2e738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Model: . Computers are getting better at understanding human language. One way they do this is by learning from a lot of text. This is called \"pre-training\". Once a computer has been pre-trained, it can be \"fine-tuned\" to do a specific task, like answering your questions or writing different kinds of text.\n",
      "\n",
      "But pre-training and fine-tuning computers can be very expensive. That's why we're working on a new way to do it that's much cheaper. Our method is called \"Low-Rank Adaptation\". It works by freezing the pre-trained model weights and injecting trainable rank decomposition matrices into each layer of the Transformer architecture. This reduces the number of trainable parameters for downstream tasks, which makes it much cheaper to fine-tune the model.\n",
      "\n",
      "We've tested our method on several different language models, and it works just as well as fine-tuning, even though it's much cheaper. We've also released a package that makes it easy to use LoRA with PyTorch models.\n"
     ]
    }
   ],
   "source": [
    "predict_large_language_model_sample(project_id = \"msca-31013-big-data\", \n",
    "                                    model_name = \"text-bison@001\", \n",
    "                                    temperature = 0.2, \n",
    "                                    max_decode_steps = 256, \n",
    "                                    top_p = 0.95, \n",
    "                                    top_k = 40, \n",
    "                                    location = \"us-central1\",\n",
    "                                    content = prompt + ' ' + text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2fadb1f-b605-4c47-8410-65bfc51bc088",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''Rewrite the following article so it can be understood by 15 year old:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09cc5558-fe94-4fc9-ad61-85dddd5fbca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Model: . One way to make computers understand human language is to teach them a lot of words and phrases. We do this by feeding them a large amount of text data, and then we make them guess what the next word in a sentence should be. This process is called \"pre-training\".\n",
      "\n",
      "Once a computer model has been pre-trained, we can then \"fine-tune\" it to perform a specific task. For example, we can fine-tune a pre-trained model to write different kinds of creative text, like poems or code.\n",
      "\n",
      "However, fine-tuning a large pre-trained model can be very computationally expensive. This is because the model has a lot of parameters, and we need to train all of them.\n",
      "\n",
      "To make fine-tuning more efficient, we can use a technique called \"low-rank adaptation\". This technique reduces the number of parameters in the model, without reducing the model's accuracy.\n",
      "\n",
      "We have developed a new low-rank adaptation method that works very well. Our method is called \"LoRA\". LoRA can reduce the number of parameters in a pre-trained model by a factor of 10,000, while still maintaining the model's accuracy.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_large_language_model_sample(project_id = \"msca-31013-big-data\", \n",
    "                                    model_name = \"text-bison@001\", \n",
    "                                    temperature = 0.2, \n",
    "                                    max_decode_steps = 256, \n",
    "                                    top_p = 0.95, \n",
    "                                    top_k = 40, \n",
    "                                    location = \"us-central1\",\n",
    "                                    content = prompt + ' ' + text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c712798-133b-47a6-9b32-bc40fe7cd9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4f57aa-2aea-4b84-bd00-cc201f9e9c75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0827c4-0288-4eca-9192-b1b7c156faeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a168c08-48e6-4429-9874-d9ad3cd822eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thu, 11 May 2023 20:36:45'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "\n",
    "datetime.datetime.now(pytz.timezone('US/Central')).strftime(\"%a, %d %B %Y %H:%M:%S\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
