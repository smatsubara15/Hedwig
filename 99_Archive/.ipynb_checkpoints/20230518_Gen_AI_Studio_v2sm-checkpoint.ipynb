{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46cc8463-76a2-4ab3-b660-298db0000931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install google-cloud-aiplatform==1.25.0\n",
    "# %pip install google-api-core==1.33.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28dc367b-a088-432f-b20d-df90a51a9aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "\n",
    "from google.cloud import storage\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc63bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! /Users/scottsmacbook/google-cloud-sdk/bin/gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04db96bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "def authenticate_implicit_with_adc(project_id=\"your-google-cloud-project-id\"):\n",
    "    \"\"\"\n",
    "    When interacting with Google Cloud Client libraries, the library can auto-detect the\n",
    "    credentials to use.\n",
    "\n",
    "    // TODO(Developer):\n",
    "    //  1. Before running this sample,\n",
    "    //  set up ADC as described in https://cloud.google.com/docs/authentication/external/set-up-adc\n",
    "    //  2. Replace the project variable.\n",
    "    //  3. Make sure that the user account or service account that you are using\n",
    "    //  has the required permissions. For this sample, you must have \"storage.buckets.list\".\n",
    "    Args:\n",
    "        project_id: The project id of your Google Cloud project.\n",
    "    \"\"\"\n",
    "\n",
    "    # This snippet demonstrates how to list buckets.\n",
    "    # *NOTE*: Replace the client created below with the client required for your application.\n",
    "    # Note that the credentials are not specified when constructing the client.\n",
    "    # Hence, the client library will look for credentials using ADC.\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    buckets = storage_client.list_buckets()\n",
    "    print(\"Buckets:\")\n",
    "    for bucket in buckets:\n",
    "        print(bucket.name)\n",
    "    print(\"Listed all storage buckets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8199b60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buckets:\n",
      "user-scripts-msca310019-capstone-49b3\n",
      "Listed all storage buckets.\n"
     ]
    }
   ],
   "source": [
    "authenticate_implicit_with_adc('msca310019-capstone-49b3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a44678fd-b78d-49d1-ac2c-abf35e4ba41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_large_language_model_sample(\n",
    "    project_id: str,\n",
    "    model_name: str,\n",
    "    temperature: float,\n",
    "    max_decode_steps: int,\n",
    "    top_p: float,\n",
    "    top_k: int,\n",
    "    content: str,\n",
    "    location: str = \"us-central1\",\n",
    "    tuned_model_name: str = \"\",\n",
    "    ) :\n",
    "    \"\"\"Predict using a Large Language Model.\"\"\"\n",
    "    vertexai.init(project=project_id, location=location)\n",
    "    model = TextGenerationModel.from_pretrained(model_name)\n",
    "    if tuned_model_name:\n",
    "        model = model.get_tuned_model(tuned_model_name)\n",
    "    response = model.predict(\n",
    "        content,\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=max_decode_steps,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "068a618a-eccc-483f-9b09-50cdc00833d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbfa5fbe-f91c-4293-ba7a-61972cd7cbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5e31232-1e0d-4636-be98-1da8975a2326",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''Rewrite the following article so it can be understood by 5 year old:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c649c9c5-0df2-494e-8412-b66793b42272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. Computers are getting better at understanding language. They can do this by learning from a lot of text. But when they learn from a lot of text, they get really big. This can make it hard to use them for some things.\\n\\nWe have a new way to make computers understand language that is smaller and faster. It works by using a special kind of math. We call it Low-Rank Adaptation, or LoRA.\\n\\nLoRA works by taking the big computer model and making it smaller. It does this by only using the parts of the model that are important for the task the computer is trying to do. This makes the computer faster and uses less memory.\\n\\nLoRA is as good as the old way of making computers understand language, but it is smaller and faster. We hope that LoRA will help computers do more things with language.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_large_language_model_sample(project_id = \"msca310019-capstone-49b3\", \n",
    "                                    model_name = \"text-bison@001\", \n",
    "                                    temperature = 0.2, \n",
    "                                    max_decode_steps = 768, \n",
    "                                    top_p = 0.8, \n",
    "                                    top_k = 40, \n",
    "                                    location = \"us-central1\",\n",
    "                                    content = prompt + ' ' + text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73852072-2fc0-4d4e-84a7-a0458e88d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''Rewrite the following article so it can be understood by 10 year old:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c32cf820-6860-4606-994b-3f422cb2e738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_large_language_model_sample(project_id = \"msca310019-capstone-49b3\", \n",
    "                                    model_name = \"text-bison@001\", \n",
    "                                    temperature = 0.2, \n",
    "                                    max_decode_steps = 256, \n",
    "                                    top_p = 0.95, \n",
    "                                    top_k = 40, \n",
    "                                    location = \"us-central1\",\n",
    "                                    content = prompt + ' ' + text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2fadb1f-b605-4c47-8410-65bfc51bc088",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''Rewrite the following article so it can be understood by 15 year old:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09cc5558-fe94-4fc9-ad61-85dddd5fbca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. One way to make computers understand human language is to teach them a lot about the world. We do this by feeding them a lot of text and then asking them questions about it. This process is called \"pre-training\". Once a computer has been pre-trained, we can then \"fine-tune\" it to do a specific task, like writing different kinds of creative text.\\n\\nThe problem is that pre-training computers to do this is very expensive. It requires a lot of computing power and time. So we need to find ways to make it cheaper.\\n\\nOne way to do this is to use a technique called \"low-rank adaptation\". This means that we only train a small part of the computer\\'s language model. This makes it much cheaper to fine-tune, but it doesn\\'t seem to affect the quality of the results.\\n\\nWe\\'ve released a package that makes it easy to use low-rank adaptation with PyTorch models. We\\'ve also trained some models using this technique and we\\'re releasing them so that other people can use them.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_large_language_model_sample(project_id = \"msca310019-capstone-49b3\", \n",
    "                                    model_name = \"text-bison@001\", \n",
    "                                    temperature = 0.2, \n",
    "                                    max_decode_steps = 256, \n",
    "                                    top_p = 0.95, \n",
    "                                    top_k = 40, \n",
    "                                    location = \"us-central1\",\n",
    "                                    content = prompt + ' ' + text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c712798-133b-47a6-9b32-bc40fe7cd9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_email = '''Create a brief response to the following email in a professional manner :'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca4f57aa-2aea-4b84-bd00-cc201f9e9c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_email = 'Hey, This week is probably not a good week, but lets definately shoot for early next week.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f0827c4-0288-4eca-9192-b1b7c156faeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_large_language_model_sample(project_id = \"msca310019-capstone-49b3\", \n",
    "                                    model_name = \"text-bison@001\", \n",
    "                                    temperature = 0.2, \n",
    "                                    max_decode_steps = 256, \n",
    "                                    top_p = 0.95, \n",
    "                                    top_k = 40, \n",
    "                                    location = \"us-central1\",\n",
    "                                    content = prompt_email + ' ' + text_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d776e775",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_email = '''Create a response to the following email in a professional manner : '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "345efd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_email = '''Hey Mike! Hope you had a great thanksgiving weekend. I used it to check out NYC! Anyway, \n",
    "                what's your take on the future of the firm? And how are the employees reacting so far? \n",
    "                It's difficult to recall a weekday when the WSJ does not carry an Enron related article. \n",
    "                And now it seems the merger is doubtful. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee19f69b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_large_language_model_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_54092/1551654030.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m predict_large_language_model_sample(project_id = \"msca310019-capstone-49b3\", \n\u001b[0m\u001b[1;32m      2\u001b[0m                                     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"text-bison@001\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                     \u001b[0mtemperature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                     \u001b[0mmax_decode_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                     \u001b[0mtop_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict_large_language_model_sample' is not defined"
     ]
    }
   ],
   "source": [
    "predict_large_language_model_sample(project_id = \"msca310019-capstone-49b3\", \n",
    "                                    model_name = \"text-bison@001\", \n",
    "                                    temperature = 0.2, \n",
    "                                    max_decode_steps = 256, \n",
    "                                    top_p = 0.95, \n",
    "                                    top_k = 40, \n",
    "                                    location = \"us-central1\",\n",
    "                                    content = prompt_email + ' ' + text_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a168c08-48e6-4429-9874-d9ad3cd822eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thu, 18 May 2023 12:03:32'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "\n",
    "datetime.datetime.now(pytz.timezone('US/Central')).strftime(\"%a, %d %B %Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dc0dbd5-75ef-4da0-b409-6f138c8bc49e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'storage' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_54092/3122794730.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_bucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'user-scripts-msca310019-capstone-49b3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'storage' is not defined"
     ]
    }
   ],
   "source": [
    "client = storage.Client()\n",
    "bucket = client.get_bucket('user-scripts-msca310019-capstone-49b3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c04fefb-7ba3-4203-9bc3-6bd327eeb0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = bucket.blob('data/data_message_reply_pairs_cleaned.csv')\n",
    "content = blob.download_as_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185bfebf-d62f-4cb4-836f-c120c6d0bb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "message_replies = pd.read_csv(io.BytesIO(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66211f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#message_replies = pd.read_csv(\"/Users/scottsmacbook/capstone/00_Data/message_reply_pairs_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67739a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_replies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5d4678",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_replies['message_list'] = message_replies['message'].apply(lambda x: str(x).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98473448",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_replies_trimmed = message_replies[message_replies.message_list.apply(lambda x: len(x)>3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6d91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(message_replies_trimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed4895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(message_replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f346bc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = list(message_replies_trimmed.message)\n",
    "generated_response = []\n",
    "\n",
    "for message in messages:\n",
    "    generated = predict_large_language_model_sample(project_id = \"msca310019-capstone-49b3\", \n",
    "                                    model_name = \"text-bison@001\", \n",
    "                                    temperature = 0.2, \n",
    "                                    max_decode_steps = 256, \n",
    "                                    top_p = 0.95, \n",
    "                                    top_k = 40, \n",
    "                                    location = \"us-central1\",\n",
    "                                    content = prompt_email + ' ' + message)\n",
    "    print(generated)\n",
    "    generated_response.append(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481f773b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(generated_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151cd9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages[2]"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m107",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m107"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
